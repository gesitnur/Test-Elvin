{"ast":null,"code":"function _classCallCheck(instance, Constructor) {\n  if (!(instance instanceof Constructor)) {\n    throw new TypeError(\"Cannot call a class as a function\");\n  }\n}\nfunction _defineProperties(target, props) {\n  for (var i = 0; i < props.length; i++) {\n    var descriptor = props[i];\n    descriptor.enumerable = descriptor.enumerable || false;\n    descriptor.configurable = true;\n    if (\"value\" in descriptor) descriptor.writable = true;\n    Object.defineProperty(target, descriptor.key, descriptor);\n  }\n}\nfunction _createClass(Constructor, protoProps, staticProps) {\n  if (protoProps) _defineProperties(Constructor.prototype, protoProps);\n  if (staticProps) _defineProperties(Constructor, staticProps);\n  return Constructor;\n}\nfunction len(blobOrArray) {\n  if (blobOrArray === undefined) return 0;\n  if (blobOrArray.size !== undefined) return blobOrArray.size;\n  return blobOrArray.length;\n}\n/*\n  Typed arrays and blobs don't have a concat method.\n  This function helps StreamSource accumulate data to reach chunkSize.\n*/\n\nfunction concat(a, b) {\n  if (a.concat) {\n    // Is `a` an Array?\n    return a.concat(b);\n  }\n  if (a instanceof Blob) {\n    return new Blob([a, b], {\n      type: a.type\n    });\n  }\n  if (a.set) {\n    // Is `a` a typed array?\n    var c = new a.constructor(a.length + b.length);\n    c.set(a);\n    c.set(b, a.length);\n    return c;\n  }\n  throw new Error('Unknown data type');\n}\nvar StreamSource = /*#__PURE__*/function () {\n  function StreamSource(reader, chunkSize) {\n    _classCallCheck(this, StreamSource);\n    this._chunkSize = chunkSize;\n    this._buffer = undefined;\n    this._bufferOffset = 0;\n    this._reader = reader;\n    this._done = false;\n  }\n  _createClass(StreamSource, [{\n    key: \"slice\",\n    value: function slice(start, end) {\n      if (start < this._bufferOffset) {\n        return Promise.reject(new Error(\"Requested data is before the reader's current offset\"));\n      }\n      return this._readUntilEnoughDataOrDone(start, end);\n    }\n  }, {\n    key: \"_readUntilEnoughDataOrDone\",\n    value: function _readUntilEnoughDataOrDone(start, end) {\n      var _this = this;\n      var hasEnoughData = end <= this._bufferOffset + len(this._buffer);\n      if (this._done || hasEnoughData) {\n        var value = this._getDataFromBuffer(start, end);\n        var done = value == null ? this._done : false;\n        return Promise.resolve({\n          value: value,\n          done: done\n        });\n      }\n      return this._reader.read().then(function (_ref) {\n        var value = _ref.value,\n          done = _ref.done;\n        if (done) {\n          _this._done = true;\n        } else if (_this._buffer === undefined) {\n          _this._buffer = value;\n        } else {\n          _this._buffer = concat(_this._buffer, value);\n        }\n        return _this._readUntilEnoughDataOrDone(start, end);\n      });\n    }\n  }, {\n    key: \"_getDataFromBuffer\",\n    value: function _getDataFromBuffer(start, end) {\n      // Remove data from buffer before `start`.\n      // Data might be reread from the buffer if an upload fails, so we can only\n      // safely delete data when it comes *before* what is currently being read.\n      if (start > this._bufferOffset) {\n        this._buffer = this._buffer.slice(start - this._bufferOffset);\n        this._bufferOffset = start;\n      } // If the buffer is empty after removing old data, all data has been read.\n\n      var hasAllDataBeenRead = len(this._buffer) === 0;\n      if (this._done && hasAllDataBeenRead) {\n        return null;\n      } // We already removed data before `start`, so we just return the first\n      // chunk from the buffer.\n\n      return this._buffer.slice(0, end - start);\n    }\n  }, {\n    key: \"close\",\n    value: function close() {\n      if (this._reader.cancel) {\n        this._reader.cancel();\n      }\n    }\n  }]);\n  return StreamSource;\n}();\nexport { StreamSource as default };","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}